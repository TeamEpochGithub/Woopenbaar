# Raggle Scraper

This module contains Python scripts for scraping and processing documents from various sources, with a focus on the Dutch Ministry of Health, Welfare and Sport (VWS) documents released under the Wet open overheid (Woo).

## Overview

The scraper package provides tools to:

- Collect documents from official government portals (primarily VWS)
- Extract document metadata and relationships
- Download documents from various public sources
- Process document metadata for further use in the Raggle system

## Components

### Main Scripts

| Script | Purpose |
|--------|---------|
| `scrape_vws.py` | Scrape documents from the VWS open data portal |
| `create_vws_metadata.py` | Process and enrich scraped metadata |
| `scrape_documents.py` | Download documents from various public URLs |
| `website_specific.py` | Handler functions for specific websites |

### Script Details

#### `scrape_vws.py`

The main script for scraping the official VWS open data portal (`open.minvws.nl`).

**Features:**
- Navigates through the VWS portal's search results pages for Woo 'besluiten' (decisions)
- Extracts metadata for both 'besluiten' and individual documents
- Downloads documents directly available on the portal
- Saves documents and metadata into specified output directories

**Usage:**
```bash
python scrape_vws.py [--output_dir OUTPUT_DIR] [--max_pages MAX_PAGES]
```

#### `create_vws_metadata.py`

Processes and enriches the metadata generated by `scrape_vws.py`, potentially combining it with existing metadata.

**Features:**
- Loads and processes scraped metadata
- Merges with existing metadata from publication reports when available
- Creates standardized metadata format for document processing
- Separates matched and unmatched documents for quality control

**Usage:**
```bash
python create_vws_metadata.py [--scraped_metadata SCRAPED_METADATA] [--output_file OUTPUT_FILE]
```

#### `scrape_documents.py`

Downloads documents from various public URLs identified during metadata processing.

**Features:**
- Handles various website structures to find document links
- Prioritizes direct PDF downloads
- Supports conversion of web pages to PDF
- Includes specialized handlers for common government websites
- Logs success and failure for each document

**Usage:**
```bash
python scrape_documents.py [--metadata_file METADATA_FILE] [--output_dir OUTPUT_DIR]
```

#### `website_specific.py`

Utility module with specialized functions for handling specific website structures.

**Features:**
- Custom extractors for sites like `tweedekamer.nl`, `officielebekendmakingen.nl`
- Web page to PDF conversion using Playwright
- Dynamic content handling for JavaScript-heavy sites

## Workflow

The typical document collection workflow is:

1. **Initial Scraping**: Run `scrape_vws.py` to collect documents and metadata from the VWS portal
2. **Metadata Processing**: Run `create_vws_metadata.py` to process and enhance the metadata
3. **Document Collection**: Run `scrape_documents.py` to download documents from external links
4. **Data Preparation**: Move collected documents and metadata to the appropriate location for processing by the `standard_data_format` module

## Output Structure

```
output/
├── besluiten/                # Decision documents (PDF)
│   └── besluit_{id}.pdf
├── inventory/                # Inventory lists (PDF)
│   └── inventory_{id}.pdf
├── documents/                # Individual documents (PDF)
│   └── doc_{id}.pdf
├── documents_metadata.csv    # Individual document metadata
└── besluiten_metadata.csv    # Decision metadata
```

## Requirements

- Python 3.8+
- Requests
- BeautifulSoup4
- Pandas
- Playwright (for web page to PDF conversion)

## Installation

```bash
pip install requests beautifulsoup4 pandas playwright

# Additional setup for Playwright
playwright install
```
