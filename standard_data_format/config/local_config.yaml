# Local inference configuration using Ollama
marker:
  use_llm: true
  output_format: "markdown"
  use_local_model: true
  llm_service: "marker.services.ollama.OllamaService"
  ollama_base_url: "http://localhost:11434"
  ollama_model: "gemma3:4b"
  service:
    name: "ollama"
    model: "gemma3:4b"
    base_url: "http://localhost:11434"
    device: "cuda:0"
  ocr:
    dpi: 400
    psm: 6
    oem: 3
  layout:
    margin_tolerance: 40
    min_line_height: 6
  debug: true
  debug_output_dir: "output_scraped/timelines/debug_output"
  force_ocr: true
  strip_existing_ocr: true
  languages: "nl"
  disable_image_extraction: false
  batch_size: 4
  sync_interval: 10
device: "cuda:1"

paths:
  base_dir: "standard_data_format"
  json_output_dir: "output/json_files"
  metadata_path: "data/metadata/vws_metadata_standardized.csv"
  document_folder: "data/woo_scraped/documents"
  base_path: "~/EpochV/WOO/rag/Raggle"

# PDF Converter Settings
pdf_converter:
  use_local_model: true
  allow_regular_conversion: true
  allow_ocr: true
  allow_llm: true
  llm_service: "marker.services.ollama.OllamaService"
  service:
    name: "ollama"
    model: "gemma3:4b"
    base_url: "http://localhost:11434"
    device: "cuda:0"
  batch_size: 4
  debug: true
  output_format: "markdown"
  languages: "nl"
  google_api_key: null
  gemini_api_key: null 